Input token: 595
Output token: 807
################################################################

LLM prompt:
train input:
[[3, 3, 0], [7, 4, 0], [0, 0, 4]]
train output:
[[3, 3, 3, 3, 3, 3, 0, 0, 0], [3, 3, 3, 3, 3, 3, 0, 0, 0], [3, 3, 3, 3, 3, 3, 0, 0, 0], [7, 7, 7, 4, 4, 4, 0, 0, 0], [7, 7, 7, 4, 4, 4, 0, 0, 0], [7, 7, 7, 4, 4, 4, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 4, 4], [0, 0, 0, 0, 0, 0, 4, 4, 4], [0, 0, 0, 0, 0, 0, 4, 4, 4]]

train input:
[[3, 0, 2], [0, 2, 2], [0, 0, 3]]
train output:
[[3, 3, 3, 0, 0, 0, 2, 2, 2], [3, 3, 3, 0, 0, 0, 2, 2, 2], [3, 3, 3, 0, 0, 0, 2, 2, 2], [0, 0, 0, 2, 2, 2, 2, 2, 2], [0, 0, 0, 2, 2, 2, 2, 2, 2], [0, 0, 0, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 3, 3, 3], [0, 0, 0, 0, 0, 0, 3, 3, 3], [0, 0, 0, 0, 0, 0, 3, 3, 3]]

test input:
[[0, 1, 0], [0, 0, 6], [6, 1, 0]]
################################################################

LLM answer:

test output:
[[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]
'''

import numpy as np
from sklearn.metrics import accuracy_score

def get_data(filename):
    with open(filename) as f:
        data = []
        for line in f:
            row = list(map(int, line.strip().split()))
            data.append(row)
    return np.array(data)

def predict(X, W):
    Y = X @ W
    return Y

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(y):
    exp_y = np.exp(y - y.max())
    return exp_y / exp_y.sum()

def cross_entropy(Y, T):
    return -np.log(T[np.argmax(Y, axis=1)]).mean()

def gradient_descent(X, Y, T, learning_rate, num_epochs):
    W = np.random.randn(*X.shape[:-1])
    for _ in range(num_epochs):
        Y_hat = predict(X, W)
        loss = cross_entropy(Y_hat, T)
        gradients = X.T @ (Y_hat - T)
        W -= learning_rate * gradients
    return W

def main():
    # Load training and test datasets
    train_input = get_data('../datasets/mnist_train_input.txt')
    train_output = get_data('../datasets/mnist_train_output.txt')
    test_input = get_data('../datasets/mnist_test_input.txt')
    test_output = get_data('../datasets/mnist_test_output.txt')

    # Define hyperparameters
    learning_rate = 0.05
    num_epochs = 100

    # Train the model
    W = gradient_descent(train_input, train_output, train_output, learning_rate, num_epochs)

    # Evaluate the model on the test dataset
    test_predictions = predict(test_input, W)
    test_accuracy = accuracy_score(test_output, test_predictions)
    print("Test Accuracy:", test_accuracy)

if __name__ == '__main__':
    main()
################################################################

Solution:
[[0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 6, 6, 6], [0, 0, 0, 0, 0, 0, 6, 6, 6], [0, 0, 0, 0, 0, 0, 6, 6, 6], [6, 6, 6, 1, 1, 1, 0, 0, 0], [6, 6, 6, 1, 1, 1, 0, 0, 0], [6, 6, 6, 1, 1, 1, 0, 0, 0]]

