TheBloke/Mistral-7B-Instruct-v0.1-GPTQ:main
Duration: 1:41:33.836485
Total: 9 / 302
Too long prompts: 254
Success log: [('239be575.json', 0.5), ('27a28665.json', 0.3333333333333333), ('44f52bb0.json', 0.5), ('794b24be.json', 1.0), ('7b7f7511.json', 1.0), ('d10ecb37.json', 1.0), ('d4469b4b.json', 0.5), ('e9afcf9a.json', 1.0), ('ff28f65a.json', 0.3333333333333333), ('1a2e2828.json', 1.0), ('b1fc8b8e.json', 0.5)]


Checked and wrong:
('445eab21.json', 1.0), 
('a87f7484.json', 1.0), 
('e872b94a.json', 1.0)


Failure log: 
Failed to run LLM for task 05f2a901.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 09629e4f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 0962bcdd.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 11852cab.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 1b60fb0c.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 1f0c79e5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 1f642eb9.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 1f876c06.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2204b7a8.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 22168020.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 22233c11.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2281f1f4.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 228f6490.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 22eb0ac0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 253bf280.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 25d487eb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 272f95fa.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 29623171.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2bcee788.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2bee17df.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 31aa019c.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3345333e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 363442ee.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 36d67576.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3906de3d.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 39a8645d.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3bdb4ada.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3befdf3e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3de23699.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3eda0437.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4093f84a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 41e4d17e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4347f46a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 444801d8.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 447fd412.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 44d8ac46.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4612dd53.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 469497ad.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4c5c2cf0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 508bd3b6.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 50cb2852.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 5117e062.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 543a7ed5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 56ff96f3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 5c0a986e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 5daaa586.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 623ea044.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 63613498.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 673ef223.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 6a1e5592.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 6d0160f0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 6e82a1ae.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7468f01a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7ddcd7ec.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7e0986d6.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7f4411dc.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 810b9b61.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 82819916.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 8403a5d5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 846bdb03.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 85c4e7cd.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 8e1813be.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 8eb1be9a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 90f3ed37.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 913fb3ed.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 91413438.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 928ad970.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 941d9a10.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 952a094c.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 95990924.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9aec4887.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9ecd008a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task a2fd1cf0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task a3325580.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task a78176bb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task a8c38be5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task af902bf9.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b27ca6d3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b548a754.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b7249182.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b782dc8a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b9b7f026.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ba97ae07.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c0f76784.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c8cbb738.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c9f8e694.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task caa06a1f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d06dbe63.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d22278a0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d2abd087.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d43fd935.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d4a91cb9.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d687bc17.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d6ad076f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d89b689b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task dbc1a6ce.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task dc0a314f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task dc433765.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ddf7fa4f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ded97339.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e21d9049.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e40b9e2f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e48d4e1a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e5062a87.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e8593010.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ea32f347.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ec883f72.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ef135b50.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f15e1fac.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f8b3ba0a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f8c80d96.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task fcb5c309.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task fcc82909.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 03560426.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 08573cc6.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 0b17323b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 0bb8deee.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 0becf7df.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 12997ef3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 136b0064.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 137f0df0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 140c817e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 19bb5feb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 1acc24af.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 1c0d0a4b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 20818e16.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 21f83797.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2685904e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2697da3f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2753e76c.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 292dd178.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 29700607.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2b01abd0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 2c737e39.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 31adaf00.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3391f8c0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 3f23242b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 423a55dc.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 42a15761.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4364c1c4.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 45737921.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 477d2879.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4acc7107.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 4e469f39.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 50a16a69.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 50aad11f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 516b51b7.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 5207a7b5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 5289ad53.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 55059096.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 575b1a71.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 5af49b42.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 604001fa.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 60a26a3e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 642248e4.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 67636eac.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 67b4a34d.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 695367ec.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 69889d6e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 6df30ad6.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 705a3229.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 712bf12e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 72a961c9.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 759f3fd3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 782b5218.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7c8af763.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7c9b52a0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7d18a6fb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7e02026e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 7ee1c6ea.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 817e6c09.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 81c0276b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 84db8fc4.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 84f2aca1.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 85fa5666.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 8719f442.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 88207623.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 8ee62060.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 917bccba.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9356391f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 94414823.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 94be5b80.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 963f59bc.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9a4bb226.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9b365c51.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9bebae7a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9c1e755f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task 9f27f097.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task a406ac07.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task a934301b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task aa300dc3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task aab50785.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ac605cbb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ad7e01d0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ae58858e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task aee291af.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b0f4d537.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b7999b51.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task b942fd60.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task baf41dbf.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task bcb3040b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task bf699163.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c35c1b4c.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c64f1187.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c658a4bd.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task c87289bb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task cad67732.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task cb227835.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ccd554ac.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ce039d91.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ce8d95cc.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task cfb2ce5a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d37a1ef5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d47aa2ff.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d492a647.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d5c634a2.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task d5c634a2.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task da2b0fe3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task da2b0fe3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task dc2aa30b.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task dd2401ed.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e0fb7511.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e1baa8a4.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e21a174a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e21a174a.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e57337a4.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e5c44e8f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e74e1818.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e7639916.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e78887d1.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e7a25a18.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e7b06bea.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e99362f0.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e9ac8c9e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task e9b4f6fc.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ea9794b1.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ecaa0ec1.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f3cdc58f.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f3e62deb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f3e62deb.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f45f5ca7.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f5aa3634.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f5c89df1.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f823c43c.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f83cb3f6.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task f9a67cb5.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task fafd9572.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task fe9372f3.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task fea12743.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

Failed to run LLM for task ff72ca3e.json. Error:
The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length function to increase the buffer size. Example:
from auto_gptq import exllama_set_max_input_length
model = exllama_set_max_input_length(model, 4096)

################################################################

