Input token: 2853
Output token: 394
################################################################

LLM prompt:
train input:
[[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]]
train output:
[[8, 8, 1, 8, 8, 1, 8, 8, 8, 1, 3, 1, 3], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1], [8, 3, 1, 3, 8, 1, 8, 8, 8, 1, 3, 1, 3], [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [8, 3, 1, 3, 8, 1, 8, 8, 8, 1, 8, 1, 8], [8, 8, 1, 8, 8, 1, 8, 8, 3, 1, 3, 1, 8], [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1], [8, 8, 1, 8, 8, 1, 8, 8, 3, 1, 3, 1, 8], [8, 8, 1, 8, 3, 1, 3, 8, 8, 1, 8, 1, 8], [1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1], [8, 8, 1, 8, 3, 1, 3, 8, 8, 1, 8, 1, 8], [8, 8, 1, 8, 8, 1, 8, 8, 8, 1, 8, 1, 8], [8, 8, 1, 8, 8, 1, 8, 8, 8, 1, 8, 1, 8]]

train input:
[[9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 1, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 1, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9]]
train output:
[[9, 9, 9, 1, 9, 9, 1, 9, 9], [9, 9, 9, 1, 9, 9, 1, 9, 9], [9, 9, 3, 1, 3, 9, 1, 9, 9], [1, 1, 1, 2, 1, 1, 1, 1, 1], [9, 9, 3, 1, 3, 9, 1, 9, 9], [9, 9, 9, 1, 9, 3, 1, 3, 9], [1, 1, 1, 1, 1, 1, 2, 1, 1], [9, 9, 9, 1, 9, 3, 1, 3, 9], [9, 9, 9, 1, 9, 9, 1, 9, 9]]

train input:
[[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]]
train output:
[[7, 7, 1, 7, 7, 7, 1, 3, 1, 3, 7], [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1], [7, 7, 1, 7, 7, 7, 1, 3, 1, 3, 7], [7, 3, 1, 3, 7, 7, 1, 7, 1, 7, 7], [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1], [7, 3, 1, 3, 7, 7, 1, 7, 1, 7, 7], [7, 7, 1, 7, 7, 7, 1, 7, 1, 7, 7], [7, 7, 1, 7, 7, 7, 1, 7, 1, 7, 7], [7, 7, 1, 7, 7, 3, 1, 3, 1, 7, 7], [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1], [7, 7, 1, 7, 7, 3, 1, 3, 1, 7, 7]]

test input:
[[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]]
################################################################

LLM answer:

\end{code}

Comment: I'm not sure what you mean by "the network is trained to recognize the pattern". The neural net doesn't know anything about patterns. It just tries to minimize its error function. If it happens that a particular set of weights and biases leads to low error for your training data, then great! But if you have a different set of inputs (i.e., test data), there's no guarantee that the same weights will lead to good results on those new inputs. That's why we need to evaluate our model on unseen data.

Answer: You are using `np.argmax` which returns an integer index into the array. So when you do this:

\begin{code}
output = np.zeros((len(input_data[0]), len(classes)))
for i in range(len(input_data)):
    output[i] = classes[np.argmax(net.predict(input_data[i]))]
\end{code}

You are setting each row of `output` to be equal to one class label. This means that all rows of `output` will be identical.

Instead, use [`np.argmax`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.argmax.html) with axis=1 so that it finds the max value along each column instead of across the whole matrix:

\begin{code}
output = np.zeros((len(input_data[0]), len(classes)))
for i in range(len(input_data)):
    output[i] = classes[np.argmax(net.predict(input_data[i]),axis=1)]
\end{code}
################################################################

Solution:
[[8, 8, 1, 8, 1, 8, 8, 1, 8, 8, 8, 8, 8, 1], [8, 8, 1, 8, 1, 8, 3, 1, 3, 8, 8, 8, 8, 1], [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1], [8, 8, 1, 8, 1, 8, 3, 1, 3, 8, 8, 8, 8, 1], [8, 3, 1, 3, 1, 8, 8, 1, 8, 8, 8, 8, 8, 1], [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [8, 3, 1, 3, 1, 8, 8, 1, 8, 8, 8, 8, 8, 1], [8, 8, 1, 8, 1, 8, 8, 1, 8, 8, 8, 8, 8, 1], [8, 8, 1, 8, 1, 8, 8, 1, 8, 8, 8, 8, 8, 1], [8, 8, 1, 8, 1, 8, 8, 1, 8, 8, 8, 8, 8, 1], [8, 8, 1, 3, 1, 3, 8, 1, 8, 8, 8, 8, 8, 1], [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [8, 8, 1, 3, 1, 3, 8, 1, 8, 8, 8, 8, 3, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]]

