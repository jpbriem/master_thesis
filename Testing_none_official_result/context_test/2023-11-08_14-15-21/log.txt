TheBloke/Llama-2-70B-GPTQ:main
Duration: 1:02:27.342318
Total: 20 / 64
Too long prompts: 2
Total input token: 120348
Total output token: 27796
Success log: []


Checked and correct but no json: -> oft sowas:"End of example.\n""",\n\t},\n}"
('239be575.json', 0.5), ('27a28665.json', 0.3333333333333333), ('29c11459.json', 1.0), ('445eab21.json', 1.0), ('44f52bb0.json', 0.5), 
('62c24649.json', 1.0), ('67e8384a.json', 1.0), ('6d0aefbc.json', 1.0), ('6fa7a44f.json', 1.0), ('794b24be.json', 0.5), ('a416b8f3.json', 1.0), ('a85d4709.json', 1.0), 
('d10ecb37.json', 1.0), ('d4469b4b.json', 0.5), ('dc433765.json', 0.5), ('e9afcf9a.json', 1.0), ('ff28f65a.json', 0.3333333333333333), ('332efdb3.json', 1.0), 
('62b74c02.json', 1.0), ('66e6c45b.json', 1.0), ('b1fc8b8e.json', 0.5), ('c8b7cc0f.json', 1.0), ('ca8de6ea.json', 1.0), 
Checked and correct but outside json/explanation:
('1bfc4729.json', 1.0), 

Checked and completly wrong:
('662c240a.json', 1.0), 
('7b7f7511.json', 1.0), 
('bbc9ae5d.json', 1.0), 
('e872b94a.json', 1.0)








Failure log: 
Failed to run LLM for task f823c43c.json. Error:
CUDA out of memory. Tried to allocate 1.81 GiB (GPU 1; 47.54 GiB total capacity; 41.80 GiB already allocated; 962.00 MiB free; 45.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

################################################################

